{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wt4dAMEyMsqT",
    "outputId": "4f26584b-b01b-46e2-b807-7a0503cc18fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Google colab link: https://colab.research.google.com/drive/1gp-AwDvdchMGJaODYjJOtaDLhfg1ifj4\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8PyL-EyN1SD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv('train-balanced-sarcasm.csv')\n",
    "train = train.dropna(subset = ['comment']) # drop na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbwXXxXwN1oF"
   },
   "outputs": [],
   "source": [
    "# without meta-data\n",
    "# for meta-data, do same as SVM and Logistic Regression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# use CountVectorizer for comparison \n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 2), stop_words = 'english', max_features = 500) \n",
    "one_hot = tfidf_vectorizer.fit_transform(train['comment']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "b9DtOgvMN1yU",
    "outputId": "1d0c8c81-0bd0-4369-e491-f7a819c59e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010773, 500)\n",
      "(1010773,)\n"
     ]
    }
   ],
   "source": [
    "print(one_hot.shape)\n",
    "y = np.asarray(train['label'])\n",
    "print(y.shape)\n",
    "np.random.seed(1001)\n",
    "mask = np.random.rand((len(y))) # for train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zdhFZY8wN14f",
    "outputId": "bb397c10-d2bd-4610-c770-61cfae7929d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([859133, 500]) torch.Size([151640, 500])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.from_numpy(one_hot[mask <= 0.85])\n",
    "x_test = torch.from_numpy(one_hot[mask > 0.85])\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OUGkHblxOXMu",
    "outputId": "ce9e95a5-d67b-4035-edf1-1c54b89c33f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([859133]) torch.Size([151640])\n"
     ]
    }
   ],
   "source": [
    "y_train = torch.from_numpy(y[mask <= 0.85])\n",
    "y_test = torch.from_numpy(y[mask > 0.85])\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clIQFEoeOXUa"
   },
   "outputs": [],
   "source": [
    "train = TensorDataset(x_train, y_train)\n",
    "test = TensorDataset(x_test, y_test)\n",
    "# Data loader\n",
    "train_loader = DataLoader(train, batch_size = 500, shuffle = True)\n",
    "test_loader = DataLoader(test, batch_size = 500, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYN5W-kzN19d"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hG87SKcyOdZ7"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "sequence_length = 25\n",
    "input_size = 20\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = 500\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8q_cELOOdkz"
   },
   "outputs": [],
   "source": [
    "# Recurrent neural network \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8HPxqJxSRDp"
   },
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "a418OrZ_SRNg",
    "outputId": "f6898e52-657c-4ca5-a10e-992504adf376",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 , Loss:  0.6657625700305408\n",
      "Epoch:  2 , Loss:  0.6517248444254833\n",
      "Epoch:  3 , Loss:  0.6472949348541801\n",
      "Epoch:  4 , Loss:  0.6444969481337272\n",
      "Epoch:  5 , Loss:  0.6423528320640932\n",
      "Epoch:  6 , Loss:  0.6406939564367031\n",
      "Epoch:  7 , Loss:  0.6390961499114312\n",
      "Epoch:  8 , Loss:  0.6378630514474994\n",
      "Epoch:  9 , Loss:  0.6364067069592345\n",
      "Epoch:  10 , Loss:  0.6352862665443798\n",
      "Epoch:  11 , Loss:  0.6340715009990024\n",
      "Epoch:  12 , Loss:  0.6329189243879757\n",
      "Epoch:  13 , Loss:  0.631758556107992\n",
      "Epoch:  14 , Loss:  0.6306844728502026\n",
      "Epoch:  15 , Loss:  0.6297587964893983\n",
      "Epoch:  16 , Loss:  0.6286483686839931\n",
      "Epoch:  17 , Loss:  0.627662649225398\n",
      "Epoch:  18 , Loss:  0.6264551293718184\n",
      "Epoch:  19 , Loss:  0.6253795435152614\n",
      "Epoch:  20 , Loss:  0.6243506327072086\n",
      "Epoch:  21 , Loss:  0.6230307885073451\n",
      "Epoch:  22 , Loss:  0.6218619414579737\n",
      "Epoch:  23 , Loss:  0.6205841630541217\n",
      "Epoch:  24 , Loss:  0.6191281058401338\n",
      "Epoch:  25 , Loss:  0.617603438582928\n",
      "Epoch:  26 , Loss:  0.6161604771231274\n",
      "Epoch:  27 , Loss:  0.6147261985240113\n",
      "Epoch:  28 , Loss:  0.6131099012787082\n",
      "Epoch:  29 , Loss:  0.6112431634505728\n",
      "Epoch:  30 , Loss:  0.6096934205643864\n",
      "Epoch:  31 , Loss:  0.6079097409939058\n",
      "Epoch:  32 , Loss:  0.6062874411205139\n",
      "Epoch:  33 , Loss:  0.6043451543184406\n",
      "Epoch:  34 , Loss:  0.6025588019009314\n",
      "Epoch:  35 , Loss:  0.6005975654334644\n",
      "Epoch:  36 , Loss:  0.5987380041917997\n",
      "Epoch:  37 , Loss:  0.5967577216998307\n",
      "Epoch:  38 , Loss:  0.5951241903904221\n",
      "Epoch:  39 , Loss:  0.592989856694728\n",
      "Epoch:  40 , Loss:  0.5912780840428781\n",
      "Epoch:  41 , Loss:  0.5895566123040905\n",
      "Epoch:  42 , Loss:  0.587916302084576\n",
      "Epoch:  43 , Loss:  0.585847993345022\n",
      "Epoch:  44 , Loss:  0.584209929620755\n",
      "Epoch:  45 , Loss:  0.5823903857725453\n",
      "Epoch:  46 , Loss:  0.5804314476033156\n",
      "Epoch:  47 , Loss:  0.5787067855770606\n",
      "Epoch:  48 , Loss:  0.5771112627175882\n",
      "Epoch:  49 , Loss:  0.5756542157683003\n",
      "Epoch:  50 , Loss:  0.5741912604349169\n",
      "Epoch:  51 , Loss:  0.5726984911633481\n",
      "Epoch:  52 , Loss:  0.5709068234955731\n",
      "Epoch:  53 , Loss:  0.569460176672611\n",
      "Epoch:  54 , Loss:  0.5678260739006367\n",
      "Epoch:  55 , Loss:  0.56652330904939\n",
      "Epoch:  56 , Loss:  0.564952895036612\n",
      "Epoch:  57 , Loss:  0.5641435965193504\n",
      "Epoch:  58 , Loss:  0.5621711403183496\n",
      "Epoch:  59 , Loss:  0.5610528194224716\n",
      "Epoch:  60 , Loss:  0.559734967158519\n",
      "Epoch:  61 , Loss:  0.5584479740464875\n",
      "Epoch:  62 , Loss:  0.5573664437448513\n",
      "Epoch:  63 , Loss:  0.5557623396369176\n",
      "Epoch:  64 , Loss:  0.5548978463864174\n",
      "Epoch:  65 , Loss:  0.5537499966768417\n",
      "Epoch:  66 , Loss:  0.5524493210221391\n",
      "Epoch:  67 , Loss:  0.5512310197840187\n",
      "Epoch:  68 , Loss:  0.550480979396688\n",
      "Epoch:  69 , Loss:  0.5489506773680985\n",
      "Epoch:  70 , Loss:  0.5481862556351833\n",
      "Epoch:  71 , Loss:  0.5472245229545758\n",
      "Epoch:  72 , Loss:  0.5461013199389848\n",
      "Epoch:  73 , Loss:  0.5448079781347822\n",
      "Epoch:  74 , Loss:  0.5443216761820118\n",
      "Epoch:  75 , Loss:  0.5427457004040046\n",
      "Epoch:  76 , Loss:  0.5426465816938856\n",
      "Epoch:  77 , Loss:  0.5417644541543747\n",
      "Epoch:  78 , Loss:  0.5406851152162901\n",
      "Epoch:  79 , Loss:  0.5398173483393094\n",
      "Epoch:  80 , Loss:  0.538490173704337\n",
      "Epoch:  81 , Loss:  0.5383781077488693\n",
      "Epoch:  82 , Loss:  0.5378273785148963\n",
      "Epoch:  83 , Loss:  0.5367191110224555\n",
      "Epoch:  84 , Loss:  0.5356520788831861\n",
      "Epoch:  85 , Loss:  0.5349168061967365\n",
      "Epoch:  86 , Loss:  0.5341556515542486\n",
      "Epoch:  87 , Loss:  0.5329673835848431\n",
      "Epoch:  88 , Loss:  0.5325461097967493\n",
      "Epoch:  89 , Loss:  0.5316938257203537\n",
      "Epoch:  90 , Loss:  0.5316107151569358\n",
      "Epoch:  91 , Loss:  0.5302793517960165\n",
      "Epoch:  92 , Loss:  0.5290922793175606\n",
      "Epoch:  93 , Loss:  0.5290236411363736\n",
      "Epoch:  94 , Loss:  0.5278034523484872\n",
      "Epoch:  95 , Loss:  0.5275576693537347\n",
      "Epoch:  96 , Loss:  0.5270621747974742\n",
      "Epoch:  97 , Loss:  0.5262604498148935\n",
      "Epoch:  98 , Loss:  0.5260593833411551\n",
      "Epoch:  99 , Loss:  0.5259746477623329\n",
      "Epoch:  100 , Loss:  0.5248074054093885\n",
      "Epoch:  101 , Loss:  0.5233074612850502\n",
      "Epoch:  102 , Loss:  0.5231233054881459\n",
      "Epoch:  103 , Loss:  0.5227364559829617\n",
      "Epoch:  104 , Loss:  0.5224928045564089\n",
      "Epoch:  105 , Loss:  0.5213765775033941\n",
      "Epoch:  106 , Loss:  0.5204941212021382\n",
      "Epoch:  107 , Loss:  0.5204670659985356\n",
      "Epoch:  108 , Loss:  0.5198575671049244\n",
      "Epoch:  109 , Loss:  0.5193758883782775\n",
      "Epoch:  110 , Loss:  0.5186088694397137\n",
      "Epoch:  111 , Loss:  0.5189457859425713\n",
      "Epoch:  112 , Loss:  0.5170136738856772\n",
      "Epoch:  113 , Loss:  0.5169561366968338\n",
      "Epoch:  114 , Loss:  0.5168294475964851\n",
      "Epoch:  115 , Loss:  0.5171211066809139\n",
      "Epoch:  116 , Loss:  0.5150621242478811\n",
      "Epoch:  117 , Loss:  0.5149728008277753\n",
      "Epoch:  118 , Loss:  0.5148429717346289\n",
      "Epoch:  119 , Loss:  0.5153665805084889\n",
      "Epoch:  120 , Loss:  0.5132526612649344\n",
      "Epoch:  121 , Loss:  0.5129337159265537\n",
      "Epoch:  122 , Loss:  0.513091841050537\n",
      "Epoch:  123 , Loss:  0.5126880335558147\n",
      "Epoch:  124 , Loss:  0.5121138278241072\n",
      "Epoch:  125 , Loss:  0.5114422702664481\n",
      "Epoch:  126 , Loss:  0.5108104245720941\n",
      "Epoch:  127 , Loss:  0.5112334433303731\n",
      "Epoch:  128 , Loss:  0.5095847122990995\n",
      "Epoch:  129 , Loss:  0.5102988082423607\n",
      "Epoch:  130 , Loss:  0.5088420458627759\n",
      "Epoch:  131 , Loss:  0.5093714565888194\n",
      "Epoch:  132 , Loss:  0.5080685856177267\n",
      "Epoch:  133 , Loss:  0.5079548021018887\n",
      "Epoch:  134 , Loss:  0.5078085030560718\n",
      "Epoch:  135 , Loss:  0.5068184253849076\n",
      "Epoch:  136 , Loss:  0.506599904060225\n",
      "Epoch:  137 , Loss:  0.5062812408123549\n",
      "Epoch:  138 , Loss:  0.5071377908857566\n",
      "Epoch:  139 , Loss:  0.504978331127716\n",
      "Epoch:  140 , Loss:  0.5056576851566841\n",
      "Epoch:  141 , Loss:  0.5047462191811961\n",
      "Epoch:  142 , Loss:  0.5058594434638576\n",
      "Epoch:  143 , Loss:  0.5034336401946049\n",
      "Epoch:  144 , Loss:  0.5040779055343804\n",
      "Epoch:  145 , Loss:  0.5029150820388705\n",
      "Epoch:  146 , Loss:  0.5031580741908954\n",
      "Epoch:  147 , Loss:  0.5033286769699399\n",
      "Epoch:  148 , Loss:  0.5036420707025799\n",
      "Epoch:  149 , Loss:  0.5020692595185895\n",
      "Epoch:  150 , Loss:  0.5011002741420317\n",
      "Epoch:  151 , Loss:  0.5020969104045626\n",
      "Epoch:  152 , Loss:  0.5008570514424309\n",
      "Epoch:  153 , Loss:  0.5006620524892147\n",
      "Epoch:  154 , Loss:  0.5004534468600887\n",
      "Epoch:  155 , Loss:  0.5011684349832873\n",
      "Epoch:  156 , Loss:  0.499013024717656\n",
      "Epoch:  157 , Loss:  0.4989908009750195\n",
      "Epoch:  158 , Loss:  0.4987724921327472\n",
      "Epoch:  159 , Loss:  0.4995442566273863\n",
      "Epoch:  160 , Loss:  0.4981412665312758\n",
      "Epoch:  161 , Loss:  0.4984990812929097\n",
      "Epoch:  162 , Loss:  0.49760524873958206\n",
      "Epoch:  163 , Loss:  0.49718156271441305\n",
      "Epoch:  164 , Loss:  0.4969853738180907\n",
      "Epoch:  165 , Loss:  0.4967829574846542\n",
      "Epoch:  166 , Loss:  0.49546034211957085\n",
      "Epoch:  167 , Loss:  0.496982213766371\n",
      "Epoch:  168 , Loss:  0.4954585114352019\n",
      "Epoch:  169 , Loss:  0.49745823247894566\n",
      "Epoch:  170 , Loss:  0.4950561840011326\n",
      "Epoch:  171 , Loss:  0.49625888825711156\n",
      "Epoch:  172 , Loss:  0.4947706365945937\n",
      "Epoch:  173 , Loss:  0.4954121530125348\n",
      "Epoch:  174 , Loss:  0.49426052860217845\n",
      "Epoch:  175 , Loss:  0.49346810698925303\n",
      "Epoch:  176 , Loss:  0.4943698879207824\n",
      "Epoch:  177 , Loss:  0.4938474835664329\n",
      "Epoch:  178 , Loss:  0.49322004797432295\n",
      "Epoch:  179 , Loss:  0.49282680452736816\n",
      "Epoch:  180 , Loss:  0.4928983929477645\n",
      "Epoch:  181 , Loss:  0.4936602189486772\n",
      "Epoch:  182 , Loss:  0.4920754952788561\n",
      "Epoch:  183 , Loss:  0.4923899479734821\n",
      "Epoch:  184 , Loss:  0.492198635319626\n",
      "Epoch:  185 , Loss:  0.49165132714607745\n",
      "Epoch:  186 , Loss:  0.49097584044954123\n",
      "Epoch:  187 , Loss:  0.49195698326656195\n",
      "Epoch:  188 , Loss:  0.4910482197670551\n",
      "Epoch:  189 , Loss:  0.49101994703716145\n",
      "Epoch:  190 , Loss:  0.4911755488978492\n",
      "Epoch:  191 , Loss:  0.4895075141274007\n",
      "Epoch:  192 , Loss:  0.4899231300858858\n",
      "Epoch:  193 , Loss:  0.49022961772410517\n",
      "Epoch:  194 , Loss:  0.4895722368486126\n",
      "Epoch:  195 , Loss:  0.48980544978203366\n",
      "Epoch:  196 , Loss:  0.48855303221833507\n",
      "Epoch:  197 , Loss:  0.48923370290662466\n",
      "Epoch:  198 , Loss:  0.4877585147687902\n",
      "Epoch:  199 , Loss:  0.4889147507027875\n",
      "Epoch:  200 , Loss:  0.4892051239877748\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "losses = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        model.to(device)\n",
    "        inputs, labels = inputs.float(), labels.long()\n",
    "        inputs = inputs.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device) # move to GPU\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(\"Epoch: \", epoch + 1, \", Loss: \", running_loss / total_step)\n",
    "    losses.append(running_loss / total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "id": "hHR8V4vRSRU6",
    "outputId": "a6cb733c-cd4f-414f-e9c5-4f5ea1205607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgI/L99iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(len(losses)) # sanity check\n",
    "epochs = []\n",
    "for i in range(1, 201): \n",
    "    epochs.append(i)\n",
    "print(len(epochs)) # sanity check\n",
    "print()\n",
    "plt.plot(epochs, losses)\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "RCQ2MCRWf4bM",
    "outputId": "1c8569b1-fa46-4465-c934-7d1aea4d5572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent Neural Network for TF-IDF vectorization:\n",
      "Training accuracy:  73.37757949002075\n",
      "Precision:  0.7994965942573572  | Recall:  0.6240694269206237  | F1-score:  0.7009739962609003\n",
      "Confusion matrix values: \n",
      "TP:  268084 , FN:  161490 , FP:  67232 , TN:  362327\n"
     ]
    }
   ],
   "source": [
    "# Performance on train set\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    tp, fp, fn, tn = 0, 0, 0, 0 # for confusion matrix\n",
    "    for i, data in enumerate(train_loader, 0): \n",
    "        inputs, labels = data \n",
    "        model.to(device) \n",
    "        inputs, labels = inputs.float(), labels.long()\n",
    "        inputs = inputs.reshape(-1, sequence_length, input_size).to(device) # move to GPU\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # true positives\n",
    "        tp += (predicted[labels == 1] == labels[labels == 1]).sum().item()\n",
    "        # false negatives\n",
    "        fn += (predicted[labels == 1] != labels[labels == 1]).sum().item()\n",
    "        # false positives\n",
    "        fp += (predicted[labels == 0] != labels[labels == 0]).sum().item()\n",
    "        # true negatives\n",
    "        tn += (predicted[labels == 0] == labels[labels == 0]).sum().item()\n",
    "        precision = (tp / (tp + fp))\n",
    "        recall =  (tp / (tp + fn))\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Recurrent Neural Network for TF-IDF vectorization:\")\n",
    "print(\"Training accuracy: \", (100 * correct) / total)\n",
    "print(\"Precision: \", precision, \" | Recall: \", recall, \" | F1-score: \", f1)\n",
    "print(\"Confusion matrix values: \")\n",
    "print(\"TP: \", tp, \", FN: \", fn, \", FP: \", fp, \", TN: \", tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "61PUd7XSf4ss",
    "outputId": "6eca3810-840c-40c1-8030-c60ec8361126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy:  68.12582432075969\n",
      "Precision:  0.7097935365196156  | Recall:  0.598457095488943  | F1-score:  0.6493877669452183\n",
      "Confusion matrix values: \n",
      "TP:  44761 , FN:  30033 , FP:  18301 , TN:  58545\n"
     ]
    }
   ],
   "source": [
    "# Performance on test set\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    tp, fp, fn, tn = 0, 0, 0, 0 # for confusion matrix\n",
    "    for i, data in enumerate(test_loader, 0): \n",
    "        inputs, labels = data \n",
    "        model.to(device) \n",
    "        inputs, labels = inputs.float(), labels.long()\n",
    "        inputs = inputs.reshape(-1, sequence_length, input_size).to(device) # move to GPU\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # true positives\n",
    "        tp += (predicted[labels == 1] == labels[labels == 1]).sum().item()\n",
    "        # false negatives\n",
    "        fn += (predicted[labels == 1] != labels[labels == 1]).sum().item()\n",
    "        # false positives\n",
    "        fp += (predicted[labels == 0] != labels[labels == 0]).sum().item()\n",
    "        # true negatives\n",
    "        tn += (predicted[labels == 0] == labels[labels == 0]).sum().item()\n",
    "        precision = (tp / (tp + fp))\n",
    "        recall =  (tp / (tp + fn))\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Testing accuracy: \", (100 * correct) / total)\n",
    "print(\"Precision: \", precision, \" | Recall: \", recall, \" | F1-score: \", f1)\n",
    "print(\"Confusion matrix values: \")\n",
    "print(\"TP: \", tp, \", FN: \", fn, \", FP: \", fp, \", TN: \", tn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Recurrent_NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
